---
title: "aggregate visualization data"
output: html_document
---



Package loading...
```{r packages}
library(ggplot2)
library(dplyr)
library(countrycode)
```


Setting local directory and suppressing scientific notation of numbers..
```{r set_directory, echo=FALSE}
# make sure local path is same as this file
getwd()
#setwd(paste0(getwd(), "/GitHub/udacity-dand-viz-project/data"))
list.files()
# suppress scientific notation, just a hack i guess...
options(scipen=999)
```


Let's import the mobile data set and add country code so that we can merge the GNI data by a common value later.
```{r mobile_import}
mobile <- read.csv("mobile-parameters-q4-2014.csv",
                   header = TRUE, sep = ",", strip.white = TRUE)

mobile$Code <- countrycode(mobile$Country, "country.name", "wb")
```


Let's add the relevant GNI data...
```{r gni_xrates_import}
gni_usd <- read.csv("world-gni-historic.csv", header = TRUE,
                           sep = ",")
```


Any missing 2014 GNI data is replaced by either 2013 or in a few rare cases 2012...
```{r gni_fill_missing}
nas <- is.na(gni_usd$X2014)
gni_usd[nas, ]$X2014 <- gni_usd[nas, ]$X2013

nas <- is.na(gni_usd$X2014)
gni_usd[nas, ]$X2014 <- gni_usd[nas, ]$X2012

# still missing a good chunk of data
nas <- is.na(gni_usd$X2014)
sum(nas)
# for these countries
as.vector(unlist(gni_usd[nas, ]$Country.Code))

gni_usd <- gni_usd %>% select(Country.Code, X2014)
```


Now we can merge the exchange rate and GNI data to the mobile data...
```{r combine_data_sets}
combine <- merge(mobile, gni_usd, by.x = "Code",
                  by.y = "Country.Code", all.x = TRUE)
```


Let's get rid of some columns...
```{r}
combine <- combine %>% select(-Pre.or.Post.paid, -Tariff.currency,
                     -Monthly.cost..specified.currency.,
                     -Pack.Cost..specified.currency., -Standard.Xrate,
                     -VAT.Rate, -Date.tariff.recorded, -Source.URL)
```


Format and rename columns...
```{r}
names(combine)
names(combine) <- c("country.code", "region", "country.name", "isp", "usage.amount",
                    "usage.unit", "usage.mb", "voice.sms", "expiry",
                    "cost.monthly.usd", "cost.usd", "cost.usd.gb", "gni")

str(combine)

combine$region <- as.character(combine$region)
combine$country.name <- as.character(combine$country.name)

combine$usage.mb <- as.numeric(as.character(combine$usage.mb))
#combine$expiry <- as.numeric(as.character(combine$expiry))
combine$cost.usd.gb <- as.numeric(as.character(combine$cost.usd.gb))
```


How many have bad MB usage and cost per gb info...
```{r}
bad.usage <- subset(combine, is.na(usage.mb))
bad.gb <- subset(combine, is.na(cost.usd.gb))
# these are both the same?
all.equal(bad.usage, bad.gb)
```


Prune the combined data...
```{r}
# usage allowance types
table(combine$usage.amount)
# make separate data table for unlimited plans and exclude from main
unlimited <- combine %>% filter(usage.amount == "Unlimited")
combine <- combine %>% filter(usage.amount != "Unlimited")

# types of voice/sms included data
table(combine$voice.sms)
# exclude any voice/sms included data
combine <- combine %>% filter(voice.sms == "")
combine <- combine %>% select(-voice.sms)


# let's see if we can find any outliers/bad data for the cost per GB
high.gb <- combine %>% filter(cost.usd.gb > 50)

# let's just filter out any data above 50 (assuming it's bad) to start and go from there...
#combine <- combine %>% filter(cost.usd.gb < 50)
```





----
```{r}
# let's look at cost per gb for each country (code) to see if we can spot outliers
# by using usage in MB and cost per GB in USD
countrys <- unique(combine$code)
# graphically
country <- combine %>% filter(code == 'AFG')
ggplot(aes(x = usage.mb, y = cost.usd.gb), data = country) +
  geom_point()

```
----





Now that we have country name together with GNI data we can add a few GNIs manually from the web...
```{r combine_manual_gni}
nas <- is.na(combine$gni)
# missing data
missing <- combine[nas, ]
# and the counts
counts <- missing %>% group_by(country.name) %>% summarise(n())

index <- combine$country.code == "AGO"
combine$gni[index] <- 6670

index <- combine$country.code == "ROM"
combine$gni[index] <- 18060

index <- combine$country.code == "SOM"
combine$gni[index] <- 125

index <- combine$country.code == "SYR"
combine$gni[index] <- 1784

index <- combine$country.code == "TWN"
combine$gni[index] <- 22598

index <- combine$country.code == "ZAR"
combine$gni[index] <- 440

# no gni data for these countries
nas <- is.na(combine$gni)
# no more missing data
sum(nas)
```


Let's get counts of different expiry types. 
```{r explore_voicesms_expiry_types}
plan.counts <- combine %>% group_by(expiry) %>% summarise(count = n()) %>%
  arrange(expiry)
```


Let's create a monthly gni column...
```{r filter_expiry_voice_plans}
combine$gni.month <- combine$gni / 12
```


Get rid of any missing cost per gb data...
```{r}
combine <- subset(combine, !is.na(cost.usd.gb))
```


Now let's group by country to get the average cost in USD per GB...
```{r final_df_costpergb}
# need to groupby 'gni.month' and 'code' here or it gets removed in the chain
final_df <- combine %>% group_by(country.code, country.name, gni.month) %>%
  summarise(count = n(), total.cost.gb.usd = sum(cost.usd.gb)) %>%
  mutate(cost.usd.per.gb = total.cost.gb.usd/count)
```


We have average cost in USD per GB but this doesn't quite paint an accurate picture mobile data accessibility.  For this we can compare the typical monthly usage (2GB) bill divided by the montly GNI per capita.  The 2GB approximate average monthly usage comes from a few online sources.  In the US, the average person consumes about 2GB of mobile data (although this doesn't include mobile wifi as in free hotspots at starbucks which is apparently close to an additional 8GB per month). This relative measure of mobile data accessibility will really in brining to life the choropleth map.
```{r final_df_percent_income}
final_df <- final_df %>%
  mutate(percent.income = 100*2*cost.usd.per.gb/gni.month) %>%
  select(-country.name, -total.cost.gb.usd, -count)
```


Let's make sure our relative measure domain is reasonable for a choropleth map...
```{r final_df_domain_check}
summary(final_df)
ggplot(aes(x = cost.usd.per.gb), data = final_df) +
  geom_histogram()
# we have some major outliers, let's see all high values...
check <- subset(final_df, cost.usd.per.gb > 30)
# let's also look at the smaller values
check <- subset(final_df, cost.usd.per.gb < 2)
# manual inspection for some bad values in each country was done on the
# original csv after this
```


Let's cut cost per GB and cost as percent of income into buckets for better visualization in d3...
```{r final_df_domain_buckets}
final_df$cost.usd.per.gb.bucket = cut(final_df$cost.usd.per.gb,
                                      c(0, 2, 5, 10, 20, 30, 50, +Inf),
                                      labels = c('0-2', '2-5', '5-10', '10-20',
                                                 '20-30', '30-50', '50-'))
final_df$cost.usd.per.gb.bucket <- as.character(final_df$cost.usd.per.gb.bucket)

final_df$percent.income.bucket = cut(final_df$percent.income,
                                      c(0, 1, 3, 5, 10, 20, 40, +Inf),
                                     labels = c('0-1', '1-3', '3-5', '5-10',
                                                 '10-20', '20-40', '40-'))
final_df$percent.income.bucket <- as.character(final_df$percent.income.bucket)
```




Write data to csv for d3 visualization...
```{r final_df_write_csv}
ordering <- c("country.code", "country.name", "gni.month", "cost.usd.per.gb",
              "percent.income", "cost.usd.per.gb.bucket",
              "percent.income.bucket")

final_df <- final_df[, ordering]

# round numbers
final_df$gni.month <- round(final_df$gni.month, 2)
final_df$cost.usd.per.gb <- round(final_df$cost.usd.per.gb, 2)
final_df$percent.income <- round(final_df$percent.income, 2)

write.csv(final_df, file = "mobile-world-data.csv", row.names = FALSE)
```
